---
layout: talk
title: "Intelligent Interaction-Oriented Multimodal Semantic Understanding"
permalink: /Speech/talk2-7/
---

<div class="talk-container">
    <div class="talk-header">
        <h2>Presenter: Prof. Zhou Zhao</h2>
    </div>
    <h3>Abstract</h3>
    <p>
    With the rapid development of information technology, multimodal data has become the main form of data in intelligent human-computer interaction （HCI） scenarios in recent years. Multimodal semantic understanding is one of the key technologies for realising human-computer intelligent interaction. Existing multimodal learning methods have some challenges in HCI scenarios, such as reduced robustness of model feature learning in general-purpose environments, failure to make full use of the parallelism of GPU hardware in the model inference stage, and insufficient real-time response of the machine in HCI. To address the above challenges in intelligent human-computer interaction-oriented scenarios, we construct a new approach to multimodal semantic understanding with "robust representation, parallel mapping and streaming inference" as its core, thereby improving the robustness of multimodal data feature encoding and speeding up model inference and decoding. This report focuses on research results of multimodal semantic understanding in different modal recognition and synthesis tasks, such as sign language,  lip language and speech in intelligent HCI scenarios.
    </p>
    <h3>Biography</h3>
    <p>
    Zhou Zhao is an associate professor and PhD supervisor at Zhejiang University. His main research interests are multimodal semantic understanding. He has published more than 60 papers in international conferences and journals, including more than 50 papers in ACM Transactions and IEEE Transactions and CCF Class A conferences. His papers have received more than 4000 Google Scholar citations. In recent years, he has chaired the National Natural Science Foundation of China and the Outstanding Young Scientists Project of Zhejiang Province.
    </p>
</div>
